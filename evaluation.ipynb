{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9de731a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "results = pd.read_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3a6326c-ca4b-46dd-a3ce-ae3b1e060514",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = results[['home advantage', 'home_team', 'away_team']]\n",
    "y = results['Result(home_team)']\n",
    "\n",
    "X = pd.get_dummies(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f553e3c-72b7-41e6-9178-4a55ac933b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT.fit(X_train, y_train)\n",
    "y_pred = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ac36d2ab-2a02-4eb8-ba2d-492f9568ff5f",
   "metadata": {},
   "source": [
    "4.1\n",
    "Mein ML Modell verwendet 3 Daten pro Zeile für die Vorhersage und das Feld \"home advantage\" ist das aussagekräftigste von allen. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2bbcad-b415-43b8-a14b-1f853a732ecc",
   "metadata": {},
   "source": [
    "4.2\n",
    "Ein guter Messmetrik für mein Modell ist der f1-score, da das Modell mehrere Klassen (Sieg, Unentschieden, Niederlage) unterscheidet und diese nicht gleich verteilt sind. Der F1-Score berücksichtigt sowohl die Genauigkeit (Precision) als auch die Trefferquote (Recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe80dc9c-5a51-41f8-b13e-488f3f376db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.49      0.50       767\n",
      "           0       0.26      0.26      0.26       513\n",
      "           1       0.61      0.63      0.62      1113\n",
      "\n",
      "    accuracy                           0.50      2393\n",
      "   macro avg       0.46      0.46      0.46      2393\n",
      "weighted avg       0.50      0.50      0.50      2393\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13acecf6-5406-421d-8d9b-edc5bf3e5ef9",
   "metadata": {},
   "source": [
    "4.3\n",
    "Mein Datensatz bzw. Modell hat eine Schwierigkeit und das ist das die Vorhersage 3 mögliche Antworten hat. Dieses Problem habe ich so gelöst, in dem ich ein code geschrieben habe, wo man für jede Möglichkeit eine Wahrheitsmatrix erstellen lassen kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aebcc51a-28bb-4be9-be77-4831456f3aa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP = 697\n",
      "FP = 416\n",
      "FN = 452\n",
      "TN = 828\n",
      "Sensitivität = 0.6066144473455178\n",
      "Spezifizität = 0.6262353998203055\n"
     ]
    }
   ],
   "source": [
    "vergleich_df = pd.DataFrame({\n",
    "    'y_test': y_test,\n",
    "    'y_pred': y_pred\n",
    "})\n",
    "\n",
    "ziel_klasse = 1\n",
    "\n",
    "# Wahrheitswerte zählen\n",
    "TP = ((vergleich_df['y_test'] == ziel_klasse) & (vergleich_df['y_pred'] == ziel_klasse)).sum()\n",
    "FN = ((vergleich_df['y_test'] == ziel_klasse) & (vergleich_df['y_pred'] != ziel_klasse)).sum()\n",
    "FP = ((vergleich_df['y_test'] != ziel_klasse) & (vergleich_df['y_pred'] == ziel_klasse)).sum()\n",
    "TN = ((vergleich_df['y_test'] != ziel_klasse) & (vergleich_df['y_pred'] != ziel_klasse)).sum()\n",
    "\n",
    "print(\"TP = \" + str(TP))\n",
    "print(\"FP = \" + str(FP))\n",
    "print(\"FN = \" + str(FN))\n",
    "print(\"TN = \" + str(TN))\n",
    "\n",
    "#Sensitivität und Spezifizität berechnen\n",
    "\n",
    "recall = TP/(TP + FN)\n",
    "precision = TP/(TP + FP)\n",
    "\n",
    "print(\"Sensitivität = \" + str(recall))\n",
    "print(\"Spezifizität = \" + str(precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730f551-f670-4dc8-b1a2-6f1131995647",
   "metadata": {},
   "source": [
    "4.4\n",
    "Mein Modell erreicht insgesamt eine mittlere Genauigkeit von 50 %. Das liegt zwar über dem Zufallsniveau (etwa 33 %) für ein Problem mit drei Klassen, ist aber meiner Meinung nach dennoch nicht besonders gut. Ich vermute, dass die geringe Genauigkeit am Datensatz liegt, da möglicherweise nicht genügend aussagekräftige Informationen für zuverlässige Vorhersagen vorhanden sind."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
